---
title: "Final - Credit Bayes"
author: "Mike Lehman"
date: "November 3, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal of this assessment is to apply a Naive Bayes classifier to a set of financial data from Germany in order to determine the likelihood that an individual will default on a loan.

Naive Bayes classifiers are simple but powerful models for assigning class labels to instances represented as a vector of feature values. Naive Bayes are known as such, as they are based on Bayesian probability and also assume that the value of a feature is indepdendent of the value of any other feature within the vector. Although Bayesion probability is often used, Naive Bayes classifiers can also apply non-Bayesian probability measures such as maximum-likelihood.

The implementation we will use can found in e1071 R package uses posterior probability. In short, posterior probability is the conditional probability of a value based on the study of previous examples and given another event with the same conditions occurred previously.

## Data Collection and Preparation

The original German credit data set can be found at the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29). At this page you will find a link for the Data Folder. Inside that folder there are several files. There are two files of note here: german.data and german.data-numeric. 

The german.data file contains categorical/symbolic attributes. The german.data-numeric file contains the same data, but represented numerically with all categorical values coded as integers. More details can be found in the german.doc file located in the same directory.

The original german.data file has been heavily modified to make our assessment easier and keep the focus on modeling and learning from the data. Each of the symbolic attributes (e.g. A11) have been converted to display/contain what each symbol represents. For example, A11 corresponds to a checking balance with less than 0 DM (German Deutsche Marks). The process to convert that data to our format has been handled for us ahead of time such that the file we will use (credit.csv)  is much easier to understand. Each of the 1000 observations from german.data are contained in credit.csv.

credit.csv is a modifed version of the german.data that contains 17 of the original 20 features found in german.data:

```{r}
library(e1071)
library(gmodels)
library(caret)

credit <- read.csv("credit.csv")
str(credit)
```

Looking at the data, we have a mix of factor and numeric/intger variables. Our Naive Bayes model will assess the default class value based on previous instances of other features and their relationship to the default class value. Because some of these numeric variables are continuous, the likelihood of the exact same value occurring frequently is probably quite low. This could have an impact on the performance of our model. 

For now, we will leave all the variable types and values untouched and build our first classifier to establish a baseline.

We will set a seed value so that our results are reproducible and randomize train and test data sets reserving 90% for training and leaving 10% for testing:

```{r}
set.seed(123)
train_sample <- sample(1000, 900)
credit_train <- credit[train_sample,]
credit_test <- credit[-train_sample,]
```

Before continuing, we'll want to double-check to make sure our train and test data sets have roughly even amounts of both possible values for default:

```{r}
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
```

The proportions are close enough to continue by assigning the corresponding train and test class labels to their own lists:

```{r}
credit_train_labels <- credit_train[,"default"]
credit_test_labels <- credit_test[,"default"]
```

## Train Model

To build our classifier we will make use of the naiveBayes() function in the e1071 R package. As mentioned earlier, this algorithm uses posterior probability to classify instances. All we need to do to contruct the classifier is to pass our train data (minus the default values) and our corresponding train labels:

```{r}
classifier <- naiveBayes(credit_train[-17], credit_train_labels)
classifier
```

The output of our classifier shows some important details about our model. It provides the a-priori (already known) values of our train labels and follows with the conditional probabilites of each feature. The numbers represent that percentage of that value occurring for each class label in the train data.

To make a prediction, we will pass our classifier and the test data set (again minus the default value).

```{r}
class_test_pred <- predict(classifier, credit_test)
```

## Evaluating Model Performance

Our predictions object contains a vector of values with factor levels of yes/no. We can compare these predictions to the 

```{r}
CrossTable(class_test_pred, credit_test_labels, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c('predicted', 'actual'))
```

The predicted class values against the actuals provide an easy to interpret table, but it is also possible to examine the probability measures that the model used to make a classification. This can be useful when comparing different models because, although two models may predict the same correct number of labels as another, the predicted probability shows how confident overall a model is with its decision:

```{r}
credit_test_prob <- predict(classifier, credit_test, type = "raw")
head(credit_test_prob)
```

This command provides us with a two dimensional list with probability estiamtes between "yes" and "no". These figures will fall between 0 and 1. We can create a data frame using these values and round the numbers off to make them more easily readable.

```{r}
credit_results <- data.frame(actual_type = credit_test_labels,
                          predict_type = class_test_pred,
                          prob_no = round(credit_test_prob[ , 1], 5),
                          prob_yes = round(credit_test_prob[ , 2], 5))
head(credit_results)
```

The output shows how confident our first model was in classifying a case as "yes" or "no". Based on our first confusion matrix, we know that our model mis-classified 24 cases. We can take advantage of our results data frame to see what the probability measures were for those cases:

```{r}
subset(credit_results, actual_type != predict_type)
```

Most of the probability estimates for these cases are relatively close, but we can see that for cases 55, 58, and 82 that the model was over 90% confident in its assignment, but predicted incorrectly. 

Finally, the caret package provides a more robust confusion matrix than that we built using gmodels. 

```{r}
confusionMatrix(credit_results$predict_type, credit_results$actual_type, positive = "yes")
```

There is wealth of detail here that can help us better understand our model. We can see a simple confusion matrix as well as our accuracy rate. Additionally, there are a few other useful metrics to use for evaluation purposes going forward.

The Kappa value tells us how well this model performed in its observed accuracy (what it correctly predicted) versus what it was expected to predict (values that any random classifier would be expected to achieve). This is very helpful at comparing multiple models and will be used heavily in the next section.

Additionally, sensitivity (true positive rate) and specificity (true negative rate) are other useful metrics to evaluate and compare different models.

## Improving Model Performance

### Laplace Estimators

For an initial improvement, we can apply a Laplacian estimator to account for rarely occurring values. In some cases there may be a value that occurs in only one sample, and nowhere else in any cases. Although this value does not occur often, its presence means that it could have some impact on the outcome of the default feature and needs to be included. Laplacian estimators assign a default value to these infrequently occurring terms, so as to increase their impact on the model. 

We can easily apply this by passing an additional parameter to our Naive Bayes model. Let's begin with a value of two and increase to four as well and compare those results using the caret package:

```{r}
# 8
laplace_8 <- naiveBayes(credit_train[-17], credit_train_labels, laplace = 8)
laplace_8_pred <- predict(laplace_8, credit_test)
confusionMatrix(laplace_8_pred, credit_test_labels, positive = "yes")

# 12
laplace_12 <- naiveBayes(credit_train[-17], credit_train_labels, laplace = 12)
laplace_12_pred <- predict(laplace_12, credit_test)
confusionMatrix(laplace_12_pred, credit_test_labels, positive = "yes")
```

Our predicted accuracy improved slightly with each of the two laplace models; however, our kappa statistic got much worse. This is likely due to overfitting that train data by having a laplace value that is somewhat irresponsibly high.

### Cross Validation

We have been using the caret package for evaluating model performance; however, the package also includes support for built-in model performance tuning on a variety of different model types. 

To see if we can yet improve upon our model we will refine the strategy we use for subsetting the data. Previously we merely created separate partitions based on percentages. Although this sounds effective in practive, it can lead to a few different problems.

First, although we did check the distribution of the class labels, there is no guarantee that the class labels will as be evenly distributed in unknown data. Secondly, we built and improved models against train data, and then evaluated performance against the test data. Ideally we would have a third set (validation) which we would use to iterate and improve models before turning them loose on test data.

The K-fold cross-validation strategy involves doing just that and iterating over a validation set before applying a model to the test data. Additionally, multiple (K) subsets are randomly generated to prevent overfitting. This approach averages overall prediction error to provide a more accurate estimate of model performance.

We will allow our models to iterate over our train split for all three stages of the process: train, validate, and test. Then to truly test predictions, we will use our test data set, which none of the models have yet seen.

There's a number of parameters we need to setup for our model. First we need to create a control object that instructs the caret function the resampling method to use, and in this case how many folds. We will use ten as that is the industry standard.

Also, we will specify a grid of tuning parameters. fL is a placeholder value for a Laplace estimator, useKernel, and adjust we will leave with the default values. We will try a few combinations of fL to see if there is any difference.

Once ready, we'll create a model using our train data sample and labels. We will inform the train() function that we are creating a Naive Bayes (nb) model, and specify our control and grid parameters:

```{r}
ctrl <- trainControl(method = "cv", number = 10)
grid <- expand.grid(.fL = 1, .usekernel = FALSE, .adjust = 1)
set.seed(123)
model = train(credit_train[-17], credit_train_labels, method = 'nb',trControl = ctrl, tuneGrid = grid)
```

To make a prediction we will pass our model to the predict() function along with the test data and create a confusion matrix to examine the results

```{r}
p <- predict(model, credit_test)
confusionMatrix(p, credit_test_labels, positive = "yes")
```

Overall this performs exactly the same as our very first model. Although we did not see any significant performance enhancements, this results should make us feel better about the consistent results we are seeing. 

Although we were unable to improve our accuracy and kappa statistic, our results are quite encouraging. A kappa of around 0.40-0.50 indicates above average performance and we can be reasonably assured that our model would likely translate well to unseen data.

## Conclusion

Using a set of consumer credit data, we were able to create a predictive model using Naive Bayes probability to predict credit defaults with above average accuracy. Although our models did score well in terms of accuracy and kappa statistics, a low sensitivity means that our model is not particularly good at finding actual loan defaults.

This is arguably a major performance inhibitor as any real-world algorithm should be able to detect these true positives at a better rate. However, overall accuracy is strong and either our first, or last, model would be a good candidate for application.

It is also worth considering the limited size of the data set and nature of the problem. 1,000 samples, while a good start, is likely to small as there are probably many unseen patterns that cannot be learned without more data. Additionally, the problem itself may be inherently difficult to classify.

Overall the final model would liekly be the better of the two candidates. Even though the very first model performed just as well, it included no cross-validation resampling strategy that can help overcome any problems of overfitting.

[R Source](credit-bayes.R) 