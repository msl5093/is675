---
title: "Final - Credit kNN"
author: "Mike Lehman"
date: "November 2, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal of this assessment is to apply a k Nearest Neighbors (kNN) classifier to a set of financial data from Germany in order to determine the likelihood that an individual will default on a loan. 

In a kNN analysis the methodology is to plot samples (training data points) within a two-dimensional feature space. The algorithm plots all known cases based on the training data during the training phase and then during the classification phase, attempts to classify an unknown feature value by plotting it within the same space and determining the closest known class feature value based on one of many possible distance formulae.

The distance between points typically is the straigh line path between the points and mostt commonly this distance is calculated using Euclidean distance. Other alternatives include Manhattan distance. The algorithm we will use applies Euclidean distance to determine proximity. Once distance is calculated, the algorithm considers the closest number of nearby points based on a pre-determined amount of points to select as decided by the user.

The closest points are the nearest neighbors in a kNN algorithm, where k represents the number of nearest neighbors to consider.

## Data Collection and Preparation

The original German credit data set can be found at the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29). At this page you will find a link for the Data Folder. Inside that folder there are several files. There are two files of note here: german.data and german.data-numeric. 

The german.data file contains categorical/symbolic attributes. The german.data-numeric file contains the same data, but represented numerically with all categorical values coded as integers. More details can be found in the german.doc file located in the same directory.

The original german.data file has been heavily modified to make our assessment easier and keep the focus on modeling and learning from the data. Each of the symbolic attributes (e.g. A11) have been converted to display/contain what each symbol represents. For example, A11 corresponds to a checking balance with less than 0 DM (German Deutsche Marks). The process to convert that data to our format has been handled for us ahead of time such that the file we will use (credit.csv)  is much easier to understand. Each of the 1000 observations from german.data are contained in credit.csv.

credit.csv is a modifed version of the german.data that contains 17 of the original 20 features found in german.data:

```{r}
credit <- read.csv("credit.csv")
str(credit)
```

kNN models are highly susceptible to large ranges in values for variables. We can see the range of the existing numeric variables:

```{r}
summary(credit[sapply(credit, is.numeric)]) 
```

From the output above, we can see the summary statistics for our numeric variables. Some of the variables appear acceptable in that they have close min and max values. However, for months loan duration, amount, and age the values range pretty wide and need to be brought closer together before performing any analysis.

To bring these values closer together we can apply a method of normalization known as min-max normalization that follows the format of: (x-min(x))/(max(x)-min(x)). To implement this, we will create a custom function:

```{r}
normalize <- function(x){
  return((x - min(x)) / (max(x) - min(x)))
}

credit["months_loan_duration"] <- lapply(credit["months_loan_duration"], normalize)
credit["amount"] <- lapply(credit["amount"], normalize)
credit["age"] <- lapply(credit["age"], normalize)

summary(credit[sapply(credit, is.numeric)])
```

Now we can see that our three problematic numeric variables have a much closer distribution.

Since we read the data in by leaving stringsAsFactors to the default of TRUE, we can see that we have mostly factor variables in our data set.

As a kNN assessment works only with numeric values we will have to convert these factor variables to a useful data type that the kNN model will be able to understand. To remedy this, we can easily cast these factors as numeric values using native R functions since each factor level is represented as an integer: 1, 2, 3, 4, etc.:

```{r}
summary(credit[sapply(credit, is.factor)])
credit$checking_balance <- as.numeric(credit$checking_balance)
credit$credit_history <- as.numeric(credit$credit_history)
credit$purpose <- as.numeric(credit$purpose)
credit$savings_balance <- as.numeric(credit$savings_balance)
credit$employment_duration <- as.numeric(credit$employment_duration)
credit$other_credit <- as.numeric(credit$other_credit)
credit$housing <- as.numeric(credit$housing)
credit$job <- as.numeric(credit$job)
credit$phone <- as.numeric(credit$phone)
str(credit)
```

There is one problem with this approach which could impact the efficacy of our classifier. Converting factor variables directly to numeric features implies that the difference in levels between the factors is consistent across all levels. That is to say, the increase in impact of moving from level one of a factor to level two is the same as the jump from level two to level three, and so on.

Looking at the job feature, the difference between management and skilled (2 and 4 factor levels in our data set) when translated to a numeric value may not mean that a skilled job is double the value of a management job. This difference may be negligible for ordinal (ordered) factors, but for nominal factors (such as job), we will simply have to concede this as a potential bias in our data when assessing the model's performance.

Now that our features are scaled, normalized, and properly coded, we will create test and train data sets for classification. We will reserve 10% of the data for testing and allow the other 90% to be used in training the model. Since the credit data is not randomly ordered, we need to ensure our train and test sets are randomized to avoid overfitting when training:

```{r}
set.seed(123)
train_sample <- sample(1000, 900)
credit_train <- credit[train_sample,]
credit_test <- credit[-train_sample,]
```

Here we set a seed value to ensure our results are repeatable and created a list of 900 random values to use to grab data from the credit set for training. We then grab the values not found in our random list to assign the test data set all the rows left from the credit set that were not used for the train set.

Lastly, let's verify that the proportion of our class feature is consistent in both the train and test sets:

```{r}
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
```

The proportions and distributions are relatively even and acceptable. Now we simply need to assign the default class feature to train and test lists and drop it from the data sets as we cannot have a factor variable in our classifier:

```{r}
credit_train_labels <- credit_train[,"default"]
credit_test_labels <- credit_test[,"default"]
credit_train <- credit_train[-17]
credit_test <- credit_test[-17]
```

This ensures that our class labels are kept consistent with the train and test data set rows before dropping the feature.

## Train Model

To train our model we will use the knn() function available in the class R package. We will pass our train data to the knn() function as well as the test data and train labels. The function will then assign a value for default (yes, no).

The final parameter to pass to the knn() function is the number of neighbors to assess when the function builds the model. The general rule of thumb in selecting the number of neighbors is to use the square root of the number of samples in the train data. In our case this means the square root of 900, which conveniently enough is exactly 30:


```{r}
library(class)
credit_test_pred <- knn(train = credit_train, test = credit_test, cl = credit_train_labels, k = 30)
```

## Evaluating Model Performance

Now that we have our kNN classifier model, we can assess performance by creating a cross tabulation to compare predicted to actual results:

```{r}
library(gmodels)
CrossTable(x = credit_test_labels, y = credit_test_pred, prop.chisq = FALSE)
```

The results indicate that our first model has performed quite well. The upper-left and bottom-right hand corner boxes indicate how often the classifier correctly predicted either a no or yes value for our default feature. The other boxes are how often the classifier incorrectly predicted the default feature value.

In this case, the classifier correctly predicted 66/67 total cases correctly where there was no default. However, it appears the classifier had some difficulty finding actual defaults and only correctly predicted 8/33 cases. Overall this puts the error rate of the classifier at: 26%. 

For a first model this performance is quite good, although identifying actual defaults correctly at a better rate would be much more useful as the real-world penalty for misclasifying actual defaults is much worse than actual not default cases.

## Improving Model Performance

In a kNN classifier, the algorithm we are using takes the class of the majority of the nearest neighbors. That is, after training, if the majority of the 30 nearest neighbors are of a default value 'no', the algorithm will classify the case as no, even though a few of the nearest neighbors may have been of the class value: 'yes'.

One way to potentially improve our algorithm is to adjust the number of nearest neighbors to consider. There are some considerations that accompany adjusting the number of neighbors. Consider too few neighbors and we may miss relevant patterns in the data, add too many and we run the risk of overfitting the data. Let's attempt to improve our classifier by slightly modifying from the k value of 30.

```{r}
credit_test_pred2 <- knn(train = credit_train, test = credit_test, cl = credit_train_labels, k = 40)
CrossTable(x = credit_test_labels, y = credit_test_pred2, prop.chisq = FALSE)
```

By increasing the number of neighbors to 40, we actually saw a slight decrease in performance as our classifier misclassified a no example leading to an additional false positive. Additionally, we did not catch any new default yes values.

As this could be a case of slight overfitting to the training data, let's decrease the number of neighbors and examine the results:

```{r}
credit_test_pred3 <- knn(train = credit_train, test = credit_test, cl = credit_train_labels, k = 20)
CrossTable(x = credit_test_labels, y = credit_test_pred3, prop.chisq = FALSE)
```

Again, no improvement and we actually matched the results using a k value of 30. Since our results didn't get worse, let's lastly try even fewer neighbors to see if that improves:

```{r}
credit_test_pred4 <- knn(train = credit_train, test = credit_test, cl = credit_train_labels, k = 10)
CrossTable(x = credit_test_labels, y = credit_test_pred4, prop.chisq = FALSE)
```

Interestingly, we see poorer performance at classifying no defaults but improved our numbers against yes defaults; however, our total error rate is worse than our initial model.

One last approach we could use is to change the way we normalized our numeric features. A min-max normalization, like that applied with our custom function, attempts to scale feature values such that their range falls between 0 and 1. Another common way of standardizing data is to scale data by assigning a z-score value to each feature example.

Z-score is simply a measure of how many standard deviations below or above the population mean a value lies. We can use native

```{r}
credit_z <- read.csv("credit.csv")
credit_z["months_loan_duration"] <- as.data.frame(scale(credit_z["months_loan_duration"]))
credit_z["amount"] <- as.data.frame(scale(credit_z["amount"]))
credit_z["age"] <- as.data.frame(scale(credit_z["age"]))

summary(credit_z[sapply(credit_z, is.numeric)]) # months_loan_duration, amount, age

credit_z$checking_balance <- as.numeric(credit_z$checking_balance)
credit_z$credit_history <- as.numeric(credit_z$credit_history)
credit_z$purpose <- as.numeric(credit_z$purpose)
credit_z$savings_balance <- as.numeric(credit_z$savings_balance)
credit_z$employment_duration <- as.numeric(credit_z$employment_duration)
credit_z$other_credit <- as.numeric(credit_z$other_credit)
credit_z$housing <- as.numeric(credit_z$housing)
credit_z$job <- as.numeric(credit_z$job)
credit_z$phone <- as.numeric(credit_z$phone)

credit_z_train <- credit_z[train_sample,]
credit_z_test <- credit_z[-train_sample,]

credit_z_train_labels <- credit_z_train[,"default"]
credit_z_test_labels <- credit_z_test[,"default"]

credit_z_train <- credit_z_train[-17]
credit_z_test <- credit_z_test[-17]

credit_z_test_pred <- knn(train = credit_z_train, test = credit_z_test, cl = credit_z_train_labels, k = 30)
CrossTable(x = credit_z_test_labels, y = credit_z_test_pred, prop.chisq = FALSE)
```

We started with a k value of 30 for our z-score scaled data set since that gave us the best performance during the first iterations. We can see some promising results here. Our total error rate is still 26%, but it appears that we have gotten a little better at identifying true positives, catching 11/33 total.

Let's try the same with a lower number of neighbors to see if we can imrove our error rate on last time:

```{r}
credit_z_test_pred <- knn(train = credit_z_train, test = credit_z_test, cl = credit_z_train_labels, k = 10)
CrossTable(x = credit_z_test_labels, y = credit_z_test_pred, prop.chisq = FALSE)
```

Not any improvement in any case here, and overall we got poorer results. Based on the combinations tested, a scaled numeric feature set using z-score and the standard starting point of 30 for k seems to have given the best results. 

## Conclusion

kNN algorithms provide a simple to understand and often very powerful supervised machine learning models. This particular machine learning technique is considered an instance-based lazy learning technique, which means that the algorithm uses examples seen in the training data as comparison values for new, unseen data. The algorithm considers new values as they compare directly to training examples. All classification is delayed until after training and the resulting model is optimized locally for that example.

Despite being a simple machine learning model, as we've seen from this assessment, powerful classifiers can be built using a kNN algorithm. Although our best error rate was 26%, and our best models did an exceptional job classifying 'no' default values, in all examples we came up short when it came to classifying actual 'yes' default values.

This difficulty could speak to the complexity of the problem in that there simply aren't enough examples in the data set to build a better model, or that predicting credit defaults is a complex problem that requires more data points than what were given.

[R Source](credit-kNN.R) 