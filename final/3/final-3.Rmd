---
title: 'Final - #3'
author: "Mike Lehman"
date: "November 23, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

## Introduction

The goal of this assessment is to compare two different machine learning regression models and their overall performance at predicting continuous outcome variables. The two methods being examined are both useful techniques for predicting numeric variables; but with some subtle differences that may impact their overall performance.

Decision trees are a family of highly extensible machine learning models which can be used for both multivariate classification analyses as well as for the prediction of continuous, numeric outcome variables. Regression trees are a subset of decision trees that can make numeric predictions by averaging the values of features when performing splits. In typical classification trees, those decision tree models split based on the distribution of categorical data. Despite the name, regression trees do not apply linear regression methods, but instead use the average feature values as mentioned for feature splitting.

Another common method for numeric prediction are linear regression models. Linear regression models seek to model the relationship between a dependent variable and one or more indpendent predictor variables. In the application of a linear model, the relationship between the predictor features and the outcome feature are assumed to be linear in nature. For every level increase of a predictor feature we should see some increase or decrease in the outcome feature. Linear regression models can be expressed as points across a two-dimensional space, with the distribution of such features indicating the relativel linearity of the features.

```{r include=FALSE}
library(psych)
library(rpart)
library(rpart.plot)
library(RWeka)
```

## Prostate Cancer Data

The data that will be used for this assessment come from a 1989 study that examined the correlation between the level of prostate specific antigen (PSA) and other clinical measures. There are a total of 97 samples which will be used to predict both the LPSA and the log cancer volume (lcavol). We will being by reading the data from the prostate.csv file into an R data frame:

```{r}
prostate <- read.csv("prostate.csv")
str(prostate)
```

The output shows us that we have a total of six features across the 97 samples. Since we will be attempting to predict both lpsa and lcavol, let's explore some of the correlation measures between all of the features to see what relationships there are between them:

```{r}
summary(prostate$lpsa)
hist(prostate$lpsa)
```

These two outputs show the distribution of our first feature, lpsa. We can see from both the summary statistics and the histogram that lpsa is very evenly distributed. It is good to know, as the distribution of a feature can impract prediction models.

We will do the same for lcavol:

```{r}
summary(prostate$lcavol)
hist(prostate$lcavol)
```

Again we see similar even distribution.

We can make use of the R package psych in order to construct a scatterplot matrix that can show us the correlation measures across all features in the data set. This can be useful to highlight any strong correlation relationships up front, and point out any other distinctions across the features:

```{r}
pairs.panels(prostate[c("lcavol", "age", "lbph", "lcp", "gleason", "lpsa")])
```

Above the diagonal of histograms, we can see the respective correlation measures between the features (row x column). Similarly, below the diagonal are scatterplots for those same relationships. In each of the scatterplots there are correlation ellipses which show correlation of the variables based on the size of the oval around the center dot which contains the mean value for the feature. The more this oval is stretched, the stronger the correlation.

Lcavol lcp seem to have a strong correlation as well as lcavol and lpsa, the two features for which we are attempting to predict:

```{r}
cor(prostate[c("lpsa", "lcavol")])
```

Correlation values fall between 0 and 1. Since we have a high value of 0.73, we know ahead of time that both of our target features are highly correlated.

Before beginning our analysis, we will have to split our data set into train and test subsets so that the train set may be used for modeling, while the test set may be used for prediction. We will randomize the data and do so along a 70/30 split since we have such a small data set to being with.

Since all of our data is numeric, we do not have to worry about data cleaning or normalization for continuous prediction:

```{r}
set.seed(123)
train_sample <- sample(97, 68)
train <- prostate[train_sample, ]
test  <- prostate[-train_sample, ]
```

## Regression Trees

### lpsa

We will begin our analysis by building a regression tree model to predict the lpsa feature as a function of all the other input features. We can utilize the rpart R package, which allows for the modeling of regression trees.

To being, we will pass our target feature (lpsa) and all other features into the rpart() function. The shorthand notation of '.' may be used to inform the function that we intend to include all other features as predictors. Finally we will specify our train data set as the data set with which to build the model:

```{r}
model.lpsa.tree <- rpart(lpsa ~ ., data = train)
model.lpsa.tree
```

The output shows use how the model split at each feature with the feature listed as well as its value relative to the feature (>, <, <=, etc.) and how many features fell into that range. Interestingly we can see that the first feature split occurred a lcavol, our second target feature for which we will be predicting later. This means that lcavol is the single most important predictor of lpsa. 

We can plot the tree using the separate rpart.plot package to visually exmaine the feature splits:

```{r}
rpart.plot(model.lpsa.tree, digits = 4, fallen.leaves = TRUE, type = 3, extra = 101)
```

In the resulting plot we can see how each feature was split (lcavol < 1.524 vs. >= 1.524) at each node.

Fallen leaves forces all leaf nodes to be aligned at the bottom and type and extra provide additional details. For example we now have a total count of samples in each leaf node instead of just the percentage.

Now we can evaluate this model's performance by making a prediction for lpsa.

```{r}
model.prediction <- predict(model.lpsa.tree, test)
```

To assess the quality of our model's predictions, we can compare the summary statistics of our test data's target feature and the predicted values:

```{r}
summary(model.prediction)
summary(test$lpsa)
```

Based on these outputs, we see that the model is not correctly indentifying the extremes, but does come close to matching the first quartile and median values.

To summarize the quality of the model we can look at the results of the correlation of the predicted versus actual values:

```{r}
cor(model.prediction, test$lpsa)
```

The correlation value measures the relationship between the predicted values to the actual values; however, to see how close the predicted values actually were, we need to apply determine the mean absolute error. The mean absolute error shows how far, on average, the predicted value is from the actual value. To do this we can create a custom function with the basic mean absolute error equation and pass our predicted and actual value sets:

```{r}
MAE <- function(actual, predicted) {
  mean(abs(actual - predicted))  
}

MAE(test$lpsa, model.prediction)
```

The result of the MAE function suggests that our model's predictions are on average about 0.72 away from the actual test values. 

The MAE is also quite far from approaching 0, which means our model is not performing particularly well. To improve our performance we can build a different type of regression tree, model trees. Model trees build out complete linear regression models for each feature split. To do so we will use the RWeka R package:

```{r}
m.m5p <- M5P(lpsa ~ ., data = train)
summary(m.m5p)

p.m5p <- predict(m.m5p, test)
summary(p.m5p)

cor(p.m5p, test$lpsa)
MAE(test$lpsa, p.m5p)
```

Although our correlation value got worse, our MAE got closer to zero, although not by much. Next we will see how regression and model trees perform at predicting lcavol.

### lcavol

```{r}
model.lcavol.tree <- rpart(lcavol ~ ., data = train, method = "anova")
model.lcavol.tree
rpart.plot(model.lcavol.tree, digits = 4, fallen.leaves = TRUE, type = 3, extra = 101)

model.prediction <- predict(model.lcavol.tree, test)
summary(model.prediction)
summary(test$lcavol)

cor(model.prediction, test$lcavol)

MAE(test$lcavol, model.prediction)
```

Similar to lpsa, our regression tree did not do a very good job at predicting its target variable. In fact, for both cases, a guess of the mean values of lpsa and lcavol for every example would have faired better.

Now we will use the same data to compare a model tree predicting lcavol against our results from our lcavol regression tree:

```{r}
m.m5p <- M5P(lcavol ~ ., data = train)
summary(m.m5p)

p.m5p <- predict(m.m5p, test)
summary(p.m5p)

cor(p.m5p, test$lcavol)
MAE(test$lcavol, p.m5p)
```

A model tree performed somewhat better when predicting lcavol. Our MAE is much closer to 0 and is nearly 0.50. A guess of the mean value of lcavol for each example would still fair better, but it appears that model trees have performed better for prediction of both lpsa and lcavol.

## Linear Regression

Previously we have seen that regression and model trees have been unable to best a guess of the mean value for every example in our data for each of our target features: lpsa and lcavol.

As previously mentioned, linear regressions attempt to model feature values based on an implied linearity between the independent and dependent features. 

We can build linear models in a similar way that we built our regression and model trees, by informing R that we intend to predict lpsa/lcavol as a function of all other features. The models will be built on the train data set and predictions ran against the test data set to assess predictive capcity.

### lpsa

```{r}
model.lpsa.lm <- lm(lpsa ~ ., data = train)
model.lpsa.lm
```

Our linear model output provides the beta coefficients for each of the target features. Again we see that lcavol itself has a high degree of impact on lpsa value. 

```{r}
summary(model.lpsa.lm)
```

The summary output of our linear model provides a number of deatils. We can see the summary statistics for residuals (difference between actual and predicted values) which do not have a very large range, but the gap is still significant.

Also, we see the p values for each features. Any value over 0.50 implies a strong degree of correlation between the independent feature and our target feature. Once again, lcavol is highly correlated, and lcp and gleason are as well.

Additionally, the R-squared and adjusted R-squared values are around 0.50 which imply that our model can account for around 58% of the variation in lpsa value.

Next let's attempt to run predictions for lpsa against the test data set:

```{r}
model.prediction <- predict(model.lpsa.lm, test)
cor(model.prediction, test$lpsa)
MAE(test$lpsa, model.prediction)
```

Our correlation value of 0.68 is strong, which is good, but our MAE is much too high. This is somewhat expected performance against new data given our relatively meager adjusted R-squared value.

### lcavol

We can follow the same procedure and build a linear model in an attempt to predict lcavol as a function of all other features:

```{r}
model.lcavol.lm <- lm(lcavol ~ ., data = train)
model.lcavol.lm
summary(model.lcavol.lm)
```

Our results are similar to our regression and model trees for lcavol prediction: lcavol is much easier to predict thatn lpsa. What may contribute to this is that gleason has a very high degree of correlation to lcavol (more so than lpsa). Although lcp, which contributed heavily to lpsa, has a very low degree of correlation to lcavol.

The R-squared and adjusted R-squared values are much more encouraging for lcavol than they were for lpsa. This seems to be in keeping with the regression and model trees.

```{r}
model.prediction <- predict(model.lcavol.lm, test)
cor(model.prediction, test$lcavol)
MAE(test$lcavol, model.prediction)
```

Predicting lcavol using our linear model, it appears that we have done a much better job on new data than when predicting lpsa. Our correlation value is much higher and our MAE is closer to 0 although is still not better than 0.50.

## Conclusion

In this assessment we have attempted to predict both lpsa and lcavol as functions of all other features in our prostate cancer data set. As we have seen with regression and model trees and basic linear models, although there is a high degree of correlation - predicted the actual values of each of these features is somewhat difficult.

It may be possible that the data set is too small at only 97 samples, or that the features simply do not have an underlying linearity. A logistic regression model or neural network may have more success as both of those methods can better model and capture non-linear relationships.

[R Source](final-3.R)