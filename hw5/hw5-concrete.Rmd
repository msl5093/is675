---
title: "IS675 HW5"
author: "Mike Lehman"
date: "October 17, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal of this assessment is to apply an Artificial Neural Network (ANN) model to a set a numeric data to predict the value of an additional numeric parameter. More generally, ANNs can be used in multiple machine learning environments and use cases. ANNs can be used in classification tasks as well predciting both linear and non-linear continuous variables. In fact, ANNs are well suited to working with highly non-linear relationships which we will soon see.

For this assessment we will be using an ANN to assess the compressive strength of concrete given a discrete set of numeric input variables. 

## Data Collection and Preparation

The original concrete data set is available at the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength). At this location follow the Data Folder link to find the original data set. The original data set is an .xslx file that contains detailed column names as to what the numbers in the table represent.

For example, we can see nine total features, including the one for which we are testing, Concrete compressive strength which is expressed in units of megapascals (MPa). Reading right-to-left starting with the compressive strength column, the next variable is age, easily understood in terms of days. 

Next, we can see that all other features are expressed in units of kilograms per cubic meter. The rest of the features are ingredients found in the concrete mixture: cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate. It is important to acquire this domain specific knowledge about the data, as the cleansed data set we will use provides only name descriptions with numeric values and not any units of measure.

It is also important to note, based on the readme file which is found in the same directory of the .xlsx file, that the values in the data set are in raw form and not scaled. This will play an important role in our data preparation as we look towards building our ANN.

We'll begin by reading the cleansed data from the concrete.csv file:

```{r}
concrete <- read.csv("concrete.csv")
str(concrete)
```

We can see the same values as those found in the original data set, but with brief names that correspond to the columns seen in the original data set.

The first problem we encounter is that many of the variables have large ranges in values:

```{r}
summary(concrete)
```

ANN's work best when input values are sclaed to a small range, typically around zero. We can scale our data by creating a custom normalize functions which applies a basic min-max normalization:

```{r}
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}
```

Now that we have our custom function, we can apply it to every value in each column of the data frame:

```{r}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
summary(concrete_norm$strength)
summary(concrete$strength)
```

The summary differences between the two data sets' taget variables show how the range has been reduced to between zero and one. This will make our ANN much more accurate with regards to predicting the target feature and dealing with the inputs.

Finally, we will subset the data into train and test sets by a 75/25 split. Based on the description of the original data set, the data are already randomly ordered, so we can simply subset by current rows:

```{r}
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
```

## Train Model

To train our model we will make use of the neuralnet R library which allows for creation of ANNs in a similar method to constructing other machine learning objects such as linear and non-linear regressions and regression trees.

We will also set a seed value so that our results can be reproduced as much as possible:

```{r}
library(neuralnet)
set.seed(12345)
```

To build our model we will pass our features (dependent and independent) to the neuralnet function and specify our train data set. 

```{r}
concrete_model <- neuralnet(formula = strength ~ cement + slag + ash + water + superplastic
                            + coarseagg + fineagg + age, data = concrete_train)
plot(concrete_model, rep="best")
```

The plot of our ANN model shows that we have an ANN with an input layer, one hidden layer (with one hidden), and an output layer. In the input layer there are eight nodes, one for each independent feature in the data set. In the hidden layer, there is a single node. 

When we created our ANN model, we did not specify a number of hidden nodes, so the neuralnet() function defaulted to 1. This will be covered in-depth in the next section as we assess and evaluate model performance.

Discuss activation functions...

The output layer contains a single node with our dependent target feature. 

There are a number of other details here that provide additional information about our ANN model. First, observe the line that connects each input feature node to the hidden layer node. Along these lines are various numbers. These numbers represent the weights for each of the connections.

In short, the weight of a connection between the input and hidden layers in an ANN is a function of backpropagation. Before discussing backpropagation in detail, the weight of a feature can be considered as a modifier for that feature. The given value for the input feature is multiplied by the weight. The weighted inputs are then used in conjunction with the hidden node's activation functions to determine the output value.

Backpropagation 

## Evaluating Model Performance

## Improving Model Performance

## Conclusion


[R Source](hw5-conrete.R) 