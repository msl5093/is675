---
title: "HW9"
author: "Mike Lehman"
date: "November 15, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal of this assessment is to explore various strategies and methods for improving the performance of machine learning models. A single machine learning model, built in a a vacuum by itself will often not perform well against new or unseen data. Despite superior performance against its own test data, there is still a strong likelihood that the model is overfitting the data set in certain areas, or at the very least that the model will not be able to identify more nuanced or hidden patterns in unknown data.

To explore the different strategies for improving model performance we will examine two key concepts: model performance tuning in general, and ensemble learning methods which seek to overcome the flaws of one inherently weak model by creating models built from multiple learners.

The German credit data set will be used to explore model performance improvements. The goal of all of the models is to predict the likelihood of a creidt default (class variable with two possible outcomes - yes/no) based on a number of numeric and categorical input variables.

The original German credit data set can be found at the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29). At this page you will find a link for the Data Folder. Inside that folder there are several files. There are two files of note here: german.data and german.data-numeric. 

The german.data file contains categorical/symbolic attributes. The german.data-numeric file contains the same data, but represented numerically with all categorical values coded as integers. More details can be found in the german.doc file located in the same directory.

The original german.data file has been heavily modified to make our assessment easier and keep the focus on modeling and learning from the data. Each of the symbolic attributes (e.g. A11) have been converted to display/contain what each symbol represents. For example, A11 corresponds to a checking balance with less than 0 DM (German Deutsche Marks). The process to convert that data to our format has been handled for us ahead of time such that the file we will use (credit.csv)  is much easier to understand. Each of the 1000 observations from german.data are contained in credit.csv.

credit.csv is a modifed version of the german.data that contains 17 of the original 20 features found in german.data:

```{r}
credit <- read.csv("credit.csv")
library(caret)
```

The caret R package will be used extensively for this assessment, so have loaded it in addition to reading our data set into R.

## Model Tuning

We will begin our discussion of improving model performance by creating a single model, but will build the model using a more extensible method which allows for performance adjustments.

The caret package contains a robust train() function which allows for setting a grid of tuning parameters prior to classification. The function will fit multiple models using a variety of input parameters, bootstrap examples, and will select the best model based on performance.

To begin, we will build a simple C5.0 decision tree using the credit data with the default attribute as the target class label:

```{r}
set.seed(300)
m <- train(default ~ ., data = credit, method = "C5.0")
m
```

From the ouput of our model object we can see the details of the various models that were built as well as some basic details about the type of learning method. The train() function considered our 1000 samples with 16 predictor variables to determine the value of the class label default.

No pre-processing of data was completed so we can make note of the fact that our features were left unchanged, which is useful information to have when considering further model improvement. For random sampling the function used bootstrapped sampling with 25 reps, the default value. 

The function provides details of ten models which were built during the training process. Two metrics are provided to assess model performance, accuracy and the kappa statistic. The output also provide how many trials were tested to find the better of weaker learners through adaptive boosting. The details for each model also indicate whether predictor winnowing (feature selection) was used and the type of model, rules based or a decision tree. 

Finally, the output also tells us which model was selected as the optimally performing model, the evaluation criteria used to select the optimal model, and the details for that model. Based on our output, it appears that the function determined a tree based model with 20 trials and a winnow value of false is the best performing model, and that the accuracy metric was used to determine model selection.

The model object we created: "m", contains a wealth of additional information, and to make predictions using this set of models, we only need to pass the object itself into R's predict() function, and it will consider the optimal model without requiring any parameters:

```{r}
p <- predict(m, credit)
table(p, credit$default)
```

We can examine the performance of the model by building a quick confusion matrix table. To dive even further, we can seethe raw class labels the model predicted as well as the probability measures for each sample:

```{r}
# predicted classes
head(predict(m, credit, type = "raw"))
# predicted probabilities
head(predict(m, credit, type = "prob"))
```

It would appear that our model has performed flawlessly; however, the same data set was used to train the model as well as make predictions, so unfortunately there is still much work to do.

### Customized Model Tuning

Our out-of-the-box caret based C5.0 decision tree model builder produced good results, but that was a series of models built on the same set of data that was used for predictions with no substantive resampling strategy outside of the default.

To further improve our caret based model tuning strategy we can supply additional parameters. First, we can define the resampling method by creating a control object using the caret function trainControl(). 

```{r}
ctrl <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
```

Here we have initialized a trainControl object that specifies the resampling strategy we will use. The type of resampling is a cross-validation resampling method with a value (number) of 10 folds. Additionally, we supplied a value for selectionFunction which specifies the tuning parameter. A value of "oneSE" means that our algorithm will select the optimal model by finding the simplest model within one standard error of the best optimal model. Using the oneSE rule, we can protect against overfitting by selecting a simpler model that is still close to the best performing model which likely overfit the data due to its complexity. 

Next, we can set a grid of tuning parameters to furter customize the tuning of our model builder. This grid will include options such as the model type to build, boosting parameters (in our case number of trials), and value for winnow:

```{r}
grid <- expand.grid(.model = "tree", .trials = c(1, 5, 10, 15, 20, 25, 30, 35), .winnow = "FALSE")
grid
```

The result of creating an object using: expand.grid means that we now have a data frame of parameters to further instruct our model builder.

Now that we have our trainControl and grid objects, we can build our train() function again, this time passing our parameterized objects to the function. In addition to providing our control and grid objects, we will also specify the evaluation metric for model selection, in this case the kappa statistic:

```{r}
set.seed(300)
m <- train(default ~ ., data = credit, method = "C5.0", metric = "Kappa", trControl = ctrl, tuneGrid = grid)
m
```

From the new output we can see the function made use of cross validation with 10 folds for resampling and built the various models with the parameters we supplied. Finally, the we can also see the model selection recap.

## Ensemble Learning

So far we have been working on tuning the performance of a single model and using the best possible model on its own. This single model may perform well against unseen data in some applications, but is very likely to suffer from a degree of bias and overfitting. 

Far better model performance can be achieved by combining multiple models (which more perform poorly individually) together through various methods to find an aggregate model with better predictive performance.

### Bagging

Bootstrap aggergating, or bagging, involves building multiple models, then having those models vote on a decision with equal weight given to each model. To further deepen the variance of the models, bagging also involves randomly drawing a subset of the training data for each model to use, then performing a final evaluation on the unseen test data. 

Since each model sees a different subset of the data, it is likely to learn or find patterns that were missed by another model. Because decision trees are highly susceptible to changes in input features, bagging is a highly recommended strategy for tuning such models.

To build a bagged decision tree model we will make use of the ipred R package. Similar to the caret train() function, we can begin by supplying only a few parameters to a function, bagging(). To start we will instruct the function of our target feature and allow it to consider all other features during training. Lastly, we will specify the number of bagging iterations for random resampling, in this case 25. 

```{r}
library(ipred)
set.seed(300)
mybag <- bagging(default ~ ., data = credit, nbagg = 25)
```

To evaluate the performance of our bagged decision tree, we can pass our bagged model to R's predict function and examine the results in a basic confusion matrix:

```{r}
credit_pred <- predict(mybag, credit)
table(credit_pred, credit$default)
```

These results are quite encouraging. It is unrealistic to expect this accuracy against unknown data, but such performance against the training data is optimistic. To see how a bagged decision tree model would work against unknown data, we can take advantage or the caret package as earilier using updated inputs:

```{r}
set.seed(300)
ctrl <- trainControl(method = "cv", number = 10)
train(default ~ ., data = credit, method = "treebag", trControl = ctrl)
```

The kapp statistic tells us that a 10-fold cross-validated bagged tree model performs at least as well as our tuned single C5.0 model from earlier in the assessment.

An ensemble method such as bagging is not limited to decision tree learners. We can apply its concepts to any number of machine learning methods.

### Boosting

While bagging is concerned with building multiple learners and taking a consensus among the models to make predictions, boosting is a similar ensemble learning method but with a different goal. Where bagging is designed to reduce variance and traing different models by changing up the feature selection, boosting is primarily concerned with creating new models that have learned from the mistakes of previous models. 

Boosting, in general, is designed to reduce bias more so than variance. Because of this it is possible that a boosted model will outperform a bagged model, but a boosted model is more suscetpible to overfitting.

Adaptive boosting as a concept involves generating weaker learners that iteratively learn from more difficult to classify examples by paying more attention to the misclassified examples. The process beings with a single classifier built on the training data. Correctly classified examples are then removed from the data set and the model building process begins again with primarily misclassified examples and continues until a minimum error threshold is reached our performance no longer improves.

AdaBoost.M1 is a decision tree based implementation of adaptive boosting and can be used in R through the adabag package.

To begin we will simply pass our features and data set into the boosting() function in the adabag package:

```{r}
library(adabag)
set.seed(300)
m_adaboost <- boosting(default ~ ., data = credit)
```

Now that we have our boosted tree model, we can run it against the predict() function:

```{r}
p_adaboost <- predict(m_adaboost, credit)
```

The object created using adabag creates an object of various data as opposed to single vector or data frame of predictions. We can access these values in the same way we would an R data frame:

```{r}
head(p_adaboost$class)
p_adaboost$confusion
```

Here we can see the class variable contains predicted class values, and the confusion variable contains a basic confusion matrix. 

The perfect predictive performance is somewhat misleading. Because adaptive boosting can continue iterating until the error rate reaches zero, the model simply continuted working on the training data until it eliminated all errors. This likely means overfitting.

For a more accurate measure of performance, we can apply random sampling again through cross-validation using 10 folds, the standard fold level for cross-validation. The adabag package includes this feature and makes it quite easily available:

```{r}
set.seed(300)
adaboost_cv <- boosting.cv(default ~ ., data = credit)
adaboost_cv$confusion
```

These are much more useful and realistic classification results. To obtain a more useful numeric quantification of our model we can use the vcd package to obtain the kappa statistic underlying the confusion matrix:

```{r}
library(vcd)
Kappa(adaboost_cv$confusion)
```

Throught the use of 10-fold cross validation and adaptive boosting, we have managed to achieve the highest kappa number so far.

### Random Forests

An additional ensemble learning technique for decision tree models that combine bootstrap aggregating with random feature selection, are random forests. Random forest implementations build a number of decision trees through random feature selection, then take a vote that is the mode of the predicted classes. Random forests can also be used in regression analysis where the mean prediction is used.

The randomForest package for R allows for easy training of a basic Random Forest learner:

```{r}
library(randomForest)
set.seed(300)
rf <- randomForest(default ~ ., data = credit)
rf
```

The default randomForest() function will train 500 trees that consider random features at each split. To determine the number of features at each split, a value can be specified that by default is the square root of all the features in the data set.

Once again, we can improve the performance of our basic Random Forest through use of caret's train() function. As before, we will create control and grid objects to fine tune the Random Forest

```{r}
# set repeated cross-validation for random sampling, repated 10 times over 10 folds
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
# set mtry values to set the number of features considered to each split
grid_rf <- expand.grid(.mtry = c(2, 4, 8, 16))
```

Now that we have set our tuning options, we can apply the train() function again, this time specifying a Random Forest model (rf) with the kappa statistic as the evaluation metric for optimal model selection and passing our grid and control parameters:

```{r}
set.seed(300)
m_rf <- train(default ~ ., data = credit, method = "rf", metric = "Kappa", trControl = ctrl, tuneGrid = grid_rf)
m_rf
```

The algorithm will generate a series of Random Forest models with repated 10-fold cross-validation as the resampling strategy based on our control parameters. Because we assigned a vector of values to the mtry parameter through our grid object the algorithm will also use a few different values for random feature selection will building the forests.

Based on the results, it appears that the model with a value of 16 for random feature selection produced the best scoring model based on the highest kappa value: 0.362.

Once again we have achieved the highest kappa statistic value so far using Random Forest's version of bagging and random feature selection along with repeated cross-validation over 10-folds.

For comparison's sake, taking what we have learned about adaptive boosting, we can revisit our C5.0 decision tree model from the beginning of the assessment and apply some basic boosting techniques to see how it compares to our Randm Forest model.

To set our tuning parameters we can construct another grid that will inform the train() function our model type will be a decision tree with trail levels of 10, 20, 30, and 40, and a winnow value of FALSE. Running multiple trials is a basic boosting technique for decision trees where multiple trees are built on the same set of features and a vote is taken as to the ultimate class label. This differs slightly from a Random Forest where random feature selection is part of the strategy.

```{r}
grid_c50 <- expand.grid(.model = "tree", .trials = c(10, 20, 30, 40), .winnow = "FALSE")
set.seed(300)
m_c50 <- train(default ~ ., data = credit, method = "C5.0", metric = "Kappa", trControl = ctrl, tuneGrid = grid_c50)
m_c50
```

The kappa performance for a boosted C5.0 decision tree did not beat our Random Forest model, but did perform admirably. The biggest advantage with a Random Forest is that random feature selection can mitigate overfitting which could be an explanation as to why a simple boosted C5.0 tree did not perform quite as well.

## Conclusion

Improving machine learning models through performance tuning can be achieved through a number of different means. We began by exploring basic strategies for tuning a single model such as bootstrap sampling to randomize the data sets exposed to the model builder. Additionally, we added resampling methods such as cross-validation to further improve out-of-the-box decision tree model performance.

To expoound on performance tuning even further, we explored two important concepts in machine learning performance: bagging and boosting. Bagging and boosting are ensemble learning methods which seek to overcome the relatively weak perforamce of a single model by aggregating multiple models in different ways. Additionally, we explored the ensemble method of Random Forests, which are themselves a form of bootstrap aggregating (bagging).

Our results indicated that for this particular problem, a Random Forest model is most likely to provide the best possible results. It is important to note however that optimal model selection and performance is highly correlated to the problem at hand, and that ultimately the decision on which machine learning model to select for use against unknown, real-world data is highly dependent on the problem at hand.

[R Source](hw9.R)