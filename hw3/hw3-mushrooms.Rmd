---
title: "IS675 HW3 - Mushrooms"
author: "Mike Lehman"
date: "October 6, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal of this assessment is to apply classification rules to a set of data to determine the value of a target feature. If decision trees are known as divide and conquer methods (recursively splittling/partitioning a data set into continually smaller sets), rule based classifiers are known as a separate and conquer heuristic. By separate and conquer, we mean to identify a rule that covers a subset of the data, then separating this partition from the rest of the data set.

In this assessment we will apply a rule based classifier to determine whether a wild mushroom species is poisonous given a finite set of characteristics. 

## Data Collection and Preparation

The original data set for our assessment can be found at the UCI machine learning data repository: [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Mushroom). At this location you will find the original raw data at the link titled: "Data Folder".

Inside the web directory, the file: agaricus-lepiota.data contains a comma-separated file with single characters that represent the data we will use for our assessment. For example, the first few characters in the first line of agaricus-lepiota.data are as follows: "p","x","s". In our data set, these same values are as follows: "poisonous","convex","smooth".

The entire list of attribute value pairs can be found in detail in the file: agaricus-lepiota.names in the same directory.

We will begin by reading the data into an R data frame from our modified .csv file:

```{r}
mushrooms <- read.csv("mushrooms.csv", stringsAsFactors = TRUE)
str(mushrooms)
```

After taking an exploratory look at the data, we can see that although the orginal source lists two levels for the feature veil_type, we can see that our data contains only one value. This was likely a data entry/recording issue and thus we need to remove this feature from our data set as it would likely lead to our rule classifier making incorrect decisions:

```{r}
mushrooms$veil_type <- NULL
```

The use of the NULL feature of R allows us to simply eliminate this feature from the data set. Next we will take a look at the distribution of our target feature to see what we are dealing with:

```{r}
table(mushrooms$type)
```

The split between poisonous and non-poisonous is fairly evenly distributed. One important distinction to note here is that before moving forward, we must consider this set of 8,214 samples to be an exhaustive list of all the wild mushrooms in the world. We cannot confidently say that this model will translate well to any unknown mushrooms that may have a different set of characterisitcs than those captured here.

## Train Model

To train our rule classifier we will be using an R based implementation of simple rule classifier. The OneR() algorithm found in the RWeka package implements a simple classification algorithm that generates one rule for each predictor in the data, then selects the rule with the smallest total error:

To utilize the OneR() method, we will pass to it our target feature and all the covariates found in the data set, and finally specify our mushrooms object as the data set:

```{r}
library(RWeka)
mushroom_1R <- OneR(type ~ ., data = mushrooms)
mushroom_1R
```

From the output above, we can see that our classifier provides a class value for our target feature based on each of the predictors in the data set. 

## Evaluating Model Performance

After outputting our model, we can also see that the classifier predicted 8004/8124 instances correctly. Let's examine the result in mor detail:

```{r}
summary(mushroom_1R)
```

From the confusion matrix, it looks like our model primarily had difficulty correctly classifying poisonous mushrooms as opposed to non-poisonous ones, a costly mistake.

Next we will see if we can improve our model to better catch such errors.

## Improving Model Performance

Also included in the RWeka package is another rule learner classifier, JRip() which is based on the RIPPER rule learning algorithm. The RIPPER algorithm is itself an improvement on the IREP (Incremental Reduced Error Pruning) algorithm, an algorithm that applies pre and post-pruning methods that grow very complex rules and prune them before separating from the fill data set.

Fortunately, we can implement this functionality through one line utilizing the JRip() method:

```{r}
mushroom_JRip <- JRip(type ~ ., data = mushrooms)
mushroom_JRip
```

We can see that our classifier learned a total of nine rules from the data set. Each rule is simple to read and understand, a powerful feature of rule based learners:

```{r}
summary(mushroom_JRip)
```

Through a summary of our model, we can see that our rule learner using RIPPER was quite successful, as it correctly classified all 8,214 instances in the data set.

For comparison's sake, let's see how an implementation of the C5.0 decision tree classifier would handle this same data set to illustrate the differences between decision tree versus rule based classifiers:

```{r}
library(C50)
mushroom_c5rules <- C5.0(type ~ odor + gill_size, data = mushrooms, rules = TRUE)
summary(mushroom_c5rules)
```

This model proved quite effective, with an error rate of only 1.4%! However, it looks like this model also had difficulty in that it misclassified 120 poisonous mushrooms as edible. Through applying a C5.0 based decision tree classifier to the same data set, we can see how much more effective rule based learners can be in dealing with certain data sets.

## Conclusion

Rule based classifiers are simple, yet effective classifiers that can discover extremely accurate rules even when given large and diverse data sets. The OneR() classifier used one rule for each of the predictors in the data set and was able to achieve 99% accuracy. The RIPPER based classifier learned a total of 9 rules from the data set, and was able to achieve 100& accuracy. 

One important distinction with rule based classifiers over decision tree classifiers is the difference between their underlying concepts: separate and conquer versus divide and conquer. When decision trees split on features, they essentially remove the feature from consideration in the rule. Rule learner classifiers can make decisions about a feature, but not completely remove the feature from the model, and can even revisit the feature in contruction of its rules.

[R Source](hw3-mushrooms.R) 