---
title: "IS675 HW4 - Insurance"
author: "Mike Lehman"
date: "October 12, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal of this assessment is to predict medical insurance expenditures for certain demographics of a population based on a number of contributing factors. To accomplish this task, we will apply a linear regression model which attempts to predict a dependent variable's value (expenses) based on a number of individual independent predictor variables. Specifically, we will be building a multiple linear regression model.

## Data Collection and Preparation

The data for this assessment can be found [here](http://zaad2.umbc.edu/class/675/Machine%20Learning%20with%20R%20-%202nd.zip). At this directory is a .zip file of data taken from the website of the publisher of the text:"Machine Learning with R", Packy Publishing.

In this .zip file there is a folder, Chapter 6, which contains the .csv file: insurance.csv. This is the source file for the data that will be used for this assessment.

Insurance.csv contains seven total variables, including integer variables age and number of children as well as a numeric variable for body mass index. Additionally, the data contain several factor variables for sex, smoker, and geographic region.

We'll begin by reading the data into an R data frame from the .csv:

```{r}
insurance <- read.csv("insurance.csv", stringsAsFactors = TRUE)
str(insurance)
```

We can set stringsAsFactos to TRUE as all of our categorical variables are indeed factors.

Next, we'll examine our target feature (dependent variable) and check to see how the data is distributed:

```{r}
summary(insurance$expenses)
hist(insurance$expenses)
```

Based on the histogram, we can see that the values for expenses are not evenly distributed. The data show a right skew as most of the values for expenses occur below 20,000. Typically a distribution like this poses a problem for a linear regression model and is worth making note of for later when evaluating the model.

There is an additional preparation step we must conduct prior to building our model. Regression models require that all features being assessed be numeric, and our data contain several categorical features. Let's begin by taking a closer look at the region variable:

```{r}
table(insurance$region)
```

The values appear to be evenly distributed across the four possible categories. This is another item to remember for later as we assess how the model handles such features.

Before we construct our model, it is often helpful to look at the correlation of the independent variables to the dependent feature. To do so, we will apply the cor() R function to our data in order to construct a correlation matrix:

```{r}
cor(insurance[c("age", "bmi", "children", "expenses")])
```

Positive correlations are listed in the above matrix as positive numbers, and negative correlations with negative numbers. The values we are interested in are represented in the first three rows in the last column of the matrix. None of the features have a strong correlation; however, age and bmi have some noticable positive correlation. 

Additionally, we can visualize these relationships by constructing a scatterplot matrix:

```{r}
pairs(insurance[c("age", "bmi", "children", "expenses")])
```

In the above scatterplot matrix, the intersection of each row and column contains a scatterplot for the variables in that row and column.

The most noticable pattern here, is that BMI and expenses seems to have a somewhat linear correlation. Also, age and expenses exhibit a pattern which shows three relatively straight lines for the relationship.

Through use of the psych R library, we can build an even more informative scatterplot matrix for our features:

```{r}
# use psych library for a more informative scatterplot matrix
library(psych)
pairs.panels(insurance[c("age", "bmi", "children", "expenses")])
```

Along the diaonal we see histograms for the distribution of the values for each feature. Above the diagonal is a correlation matix for each row by column, and below the diagonal are enhanced scatterplots for each of the relationships by row and column. In each of the scatterplots there are now correlation ellipses which show correlation of the variables based on the size of the oval around the center dot which contains the mean value for the feature. The more this oval is stretched, the stronger the correlation.

Additionally, the scatterplots now display a loess curve which indicates the general relationship between the x and y axis variables. The loess curve for age and BMI gradually slopes up, which implies that BMI increases with age. 

These enhanced scatterplots can provide some additional, useful correlation insights into the feature relationships.

## Train Model

Building our linear regression model is fairly straighforward in R. To do so, we simply call the lm() function and pass our data and features into the function:

```{r}
ins_model <- lm(expenses ~ ., data = insurance)
```

The first value passed is the dependent feature we are attempting to model, followed by a tilde and a list of each independent feature, separated by '+' signs. Since we are including all of the features, we can simply provide the '.' character after the tilde. This tells the function to take all independent features from the data set. Lastly, we provide the data set.

Now that we have our model, let's examine the results by passing the model to the R interpreter:

```{r}
ins_model
```

The output shows the estimated beta coefficients for our model. Basically, these values show that for every one unit increase in the feature listed, we can expect an increase or decrease in the amount specified for the dependent feature, assuming all other values are constant.

Our model was also able to handle the non-numeric features by dummy coding the values and creating variables to stand in for those. For example, sex is a factor with possible values of male or female. The model assigned a value of 1 for male and 0 for female. Similar dummy coding was performed on the region feature and smoker. 

The model leaves one category (value) out to serve as a referenece and interprets the remaining features relative to those that are left out. For example, smoker - no, sex - female, and region - northeast are left out, meaning female smokers in the northeast are the reference group. 

## Evaluating Model Performance

To evaluate the performance of our model, we simply need to call the summary() function and pass our model to the function:

```{r}
summary(ins_model)
```

In the resulting output we can parse these details for the important values that indicate the strength of our model.

First, we can see summary statistics for residuals (differnece between predicted values and actuals). With a max value of 29981.7, this suggests the the model under-predicted expenses by almost $30,000. 

Next we can see the coefficients listed below the residual summary statistics. Examining the p values for the coefficients, we can see that several of the features have a significant impact. Age, BMI and smokeryes all have noticably high p values. The higher a p value, the more likely the feature has some relationship with the dependent variable.

Finally, the R-squared values provide an indicator of overall performance of the model. Similar to the correlation coefficient, the closer the R-squared value to 1, the better the model predicts the target feature. The multiple R-squared for our model at 0.7509 means that our model explans approximately 75% of the variations in expenses. 

Models with muultiple features nearly always explain more variation than those with fewer, so the adjusted R-squared value penalizes models with many features. Our adjusted R-squared is 0.7494. This is useful when comparing performance of models with different numbers of features.

## Improving Model Performance

Our linear regression model performed quite well. With only a few R functions we were able to make powerful obervations about the data. One key aspect of linear regression models is that they leave feature selection up to the user. Since we have some basic a priori knowledge about the data, this means that we can improve our model by engineering feature properties that may help improve our model's performance.

In a linear regression, it is assumed that the relationship between an independent feature and the dependent variable is assumed to be linear; however, this is not always the case. For example, increases in age may not have a constant impact on expenses. As age gets higher the impact on expenses may be exponentially greater than a straight linear pattern.

To add this to our model, we can add a new feature that is our age variable raised to the power of two to represent a non-linear relationship:

```{r}
insurance$age2 <- insurance$age^2
```

Looking at our features, there may be other changes we can make to improve our model. For certain numeric variables, it may be the case that the effect on the dependent feature is not cumulative, but rather only has an impact once its value crosses a certain threshold. For example, a lower BMI may have not real impact on expenses; however, a BMI of greater than 30 may have a noticable impact. To model this binary assertion we can create an additional new feature that assigns 1 for BMI over 30 and 0 for BMI under 30:

```{r}
insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)
```

Finally, there are cases where multiple features may have a greater impact on the dependent variable when combined than when considered individually, as is the default case with a linear regression model. For example, having a high BMI and being a smoker may have a significant impact than those two variables considered individually.

To model this interaction we simply pass our bmi30 feature to the lm() function with a '*' and the smoker feature when building the model:

```{r}
ins_model2 <- lm(expenses ~ age + age2 + children + bmi + sex + bmi30*smoker + region, data = insurance)
```

Now the we have our transformed model, we can observe the changes and compare it to our base model:

```{r}
summary(ins_model2)
```

From the model summary we can see that our new features and our interaction had a significant impact and helped our model's scores improve. Our R-squared and adjust R-squared values are much closer to 1: 0.8664 and 0.8653 respectively. The max value for residuals has also decreased.

Additionally, our squared age feature has a very large p value as well as the bmi30:smokeryes interaction. 

Let's try one additional model. Going back to the beginning of our assessment, recall that the dependent variable (expenses) had an uneven distribution of values. We can apply a logarithmic transformation (with a base value of 10 - a common value) to bring the values closer together to see if this has any impact on our model:

```{r}
ins_model3 <- lm(log(expenses, base = 10) ~ age + age2 + children + bmi + sex + bmi30*smoker + region, data = insurance)
summary(ins_model3)
```

Our feature's p values largely held constant relative to the other models; however, our R-squared values did not score as well. This suggests that the uneven distribution of the dependent variable probably does not have that much of an impact on the model.

## Conclusion

Using linear regression modeling we were able to predict health insurance expenditures with an 80%+ degree of accuracy. For linear models this is quite good as many models do not achieve numbers this accurate. Of course this was built on a relatively small data set, but our model was still able to identify the strongest correlations between the independent features and the dependent variable.


[R Source]() 